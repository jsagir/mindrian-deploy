Let me challenge your thinking:

> What if a QA checklist didn't just *list features* â€” but actually *taught you how to think* about what's changing in your system?

That's what we're going to do here.

---

## FRAME THE JOURNEY

**What are we trying to understand?**
Whether the system behaves as intended after 27+ major codebase changes.

**Why does it matter?**
Because the illusion of progress is dangerous. Features aren't valuable if they aren't understood, tested, or used correctly.

**What happens if we get it wrong?**
Subtle failures hide in plain sight. UX confuses users. Bots act unpredictably. Confidence collapses.

**What will we be able to do after this?**
Systematically test each new behavior â€” not just *that* something changed, but *why*, *how*, and *what to expect next*.

---

# QA Review Guide: Thinking Through Change

**Branch:** `claude/review-codebase-nRTQ3`
**Commits:** 27+
**Lines:** +15,000
**Date:** Jan 29â€“30, 2026

---

## ðŸ” FEATURE 1: Extended Thinking UI

> *"What happens when a bot shows its work?"*

**Before:** Responses just appeared.
**Now:** A collapsible "ðŸ§  Thinking" panel appears before every answer (in most bots).

**Test for Understanding:**

* Does the panel appear *before* the bot speaks?
* Do you see âœ…, ðŸ”„, or âš ï¸ status indicators?
* Does Lawrence mode **not** show it?

**Scenarios:**

1. TTA â†’ "What if everyone worked remotely forever?" â†’ ðŸ§  shows reasoning steps.
2. Red Team â†’ "Our customers will definitely pay $100/month" â†’ âš ï¸ Assumptions flagged.
3. Lawrence â†’ "Help me explore a business idea" â†’ NO panel (by design).

---

## ðŸ§­ FEATURE 2: Cynefin-Aware Agent Suggestions

> *"What if the system could diagnose the *type* of problem you're facing?"*

Now it does. Based on the Cynefin framework:

| Problem Type | Bot Suggested |
| ------------ | ------------- |
| Simple       | Ackoff        |
| Complicated  | S-Curve, JTBD |
| Complex      | TTA, Scenario |
| Chaotic      | Red Team      |

**Test:**

* Lawrence â†’ "Everything's breaking!" â†’ Suggests Red Team.
* Lawrence â†’ "What's education in 20 years?" â†’ Suggests TTA.
* Lawrence â†’ "When should we launch?" â†’ Suggests S-Curve.

---

## â“ FEATURE 3: Beautiful Questions Auto-Trigger

> *"Are we asking the right question at the right time?"*

System now detects whether you're in a:

* **WHY** phase (certainty, no problem) â†’ triggers BQ bot
* **WHAT IF** phase (exploring) â†’ triggers TTA
* **HOW** phase (solutions) â†’ triggers planning tools

**Test:**

* "AI will definitely replace most jobs" â†’ Beautiful Questions bot should appear.

---

## ðŸ›‘ FEATURE 4: Extreme Opposition Mode

> *"Can you survive a Red Team attack?"*

**New toggle**: "ðŸ”´ Extreme Opposition"
Once activated, Red Team drops all balance and attacks relentlessly.

**Test Cases:**

1. Turn it on â†’ "EXTREME OPPOSITION MODE ACTIVATED"
2. Say: "Sell premium bottled water" â†’ Pure critique, no compliments.
3. Turn it off â†’ "Normal Mode Restored"

---

## ðŸ§  FEATURE 5: AI-Powered Research Synthesis

> *"What if your bot could read like a grad student?"*

"Deep Research" now returns:

* Relevance scores (0.0â€“1.0)
* Source authority rankings
* Framed via the PWS model

**Test:**

1. TTA â†’ "Explore plant-based meat" â†’ Click **Deep Research**
2. Watch the spinner â†’ Get results with scores and insights.

---

## ðŸŒ€ FEATURE 6: Smart Phase Transitions

> *"Don't move forward just because you can â€” do it because you're ready."*

**New behaviors:**

* Shows % complete before "Next Phase"
* Warns if deliverables are missing
* Offers "Proceed Anyway" override

**Test:**

* Say "hi" â†’ click Next Phase â†’ See progress status and override.
* Click Next Phase multiple times, switch bots â†’ Progress should persist.

---

## ðŸ” FEATURE 7: Show Example Pipeline

> *"Examples are only helpful if they're *different* every time."*

"Show Example" pulls from 3 systems (LazyGraph â†’ File Search â†’ Tavily), and tracks what you've already seen.

**Test:**

* TTA â†’ Click "Show Example" 3 times â†’ Each example should be new.

---

## ðŸŽ™ FEATURE 8: Voice Input

> *"Say it out loud. Let the system listen."*

**Test:**

1. Click the mic icon (bottom left)
2. Speak: "What is a problem worth solving?"
3. Check for correct transcription and thoughtful response.

---

## ðŸŽ“ FEATURE 9: Problem Discovery Grading Agent (NEW)

> *"What if we could objectively measure whether students found REAL problems?"*

This is the most sophisticated new addition. A complete grading pipeline that evaluates student work on problem discovery methodology.

**The Core Question:**
Did they find problems that are REAL, or just assumptions dressed up as insights?

**Grading Formula:**
```
Discovery_Score = 0.35Ã—PR + 0.25Ã—PD + 0.20Ã—FI + 0.10Ã—MT + 0.05Ã—CW + 0.05Ã—IW
```

| Component | Weight | What It Measures |
|-----------|--------|------------------|
| Problem Reality (PR) | 35% | "Is it Real?" â€” evidence vs. assumptions |
| Problem Discovery (PD) | 25% | How many validated problems found? |
| Framework Integration (FI) | 20% | Did they use PWS tools correctly? |
| Mindrian Thinking (MT) | 10% | Did they find hidden connections? |
| Can We Win (CW) | 5% | Basic capability check |
| Is it Worth It (IW) | 5% | Basic market sizing |

**The 8-Phase Pipeline:**

1. **Bias Detection (MANDATORY)** â€” 7 critical biases checked before grading proceeds
2. **Domain Analysis** â€” What domains are addressed vs. overlooked?
3. **Framework Validation** â€” Were JTBD, TTA, 5 Whys used correctly?
4. **Problem Extraction** â€” Classify: validated / assumed / fantasy
5. **Evidence Quality** â€” User quotes? Frequency data? Root cause?
6. **Score Calculation** â€” Apply weighted formula
7. **Quality Validation** â€” All thresholds met?
8. **Report Generation** â€” Full structured output

**Test Scenarios:**

1. **Access the Grading Bot**
   - Switch to "Problem Discovery Grading" in chat profiles
   - See the welcome message explaining the grading approach

2. **Full Grade Pipeline**
   - Upload a student PDF/DOCX
   - Click "Grade Student Work" or say "I want to grade student work"
   - Watch the pipeline phases execute
   - LOOK FOR:
     - Bias detection completes (>= 75% confidence required)
     - Grade breakdown table appears
     - PWS quality scores shown
     - JSON handoff generated

3. **Quick Grade**
   - Upload a document
   - Click "Quick Grade"
   - LOOK FOR:
     - Faster result (no full pipeline)
     - Letter grade and verdict
     - Option to run full grade

4. **Bias Detection Blocking**
   - Upload obviously biased work
   - LOOK FOR:
     - "Grading Blocked" message if confidence < 75%
     - List of detected biases
     - Cannot proceed without addressing

5. **Export & Explain**
   - After grading completes
   - Click "Export JSON" â†’ structured data
   - Click "Explain This Grade" â†’ detailed rationale

**Critical Understanding:**
Finding ONE deeply validated real problem is worth more than 20 assumed problems. The grading reflects this.

---

## ðŸ“Š FEATURE 10: Claims-Aware Presentation Analyzer (NEW)

> *"What if PDFs weren't just text dumps â€” but structured arguments to dissect?"*

This feature transforms how we analyze uploaded presentations and PDFs.

**Before:** Extract text, lose all visual context.
**Now:** Convert PDF pages to images, analyze visually with Gemini multimodal, extract claims with PWS methodology.

**What Gets Extracted:**

| Category | Examples |
|----------|----------|
| Primary Thesis | The main argument in one sentence |
| Key Claims | Supporting arguments (numbered) |
| Evidence Anchors | Data, research, quotes supporting claims |
| Presenters | Names, roles, affiliations |
| Research Citations | Academic references, studies |
| Links & Resources | URLs, external references |
| Key Data Points | Statistics, metrics, numbers |

**PWS Analysis Layer (via LangExtract):**

| PWS Field | What It Catches |
|-----------|-----------------|
| Core Problem | Main problem being addressed |
| Stated Assumptions | Explicit "we assume..." statements |
| Hidden Assumptions | Implied but unstated assumptions |
| Potential Biases | Confirmation bias, survivorship, etc. |
| Open Questions | What remains unanswered |
| PWS Quality Scores | Problem clarity, data grounding, assumption awareness (0-10) |
| Coaching Hint | Invisible guidance for follow-up |

**Test Scenarios:**

1. **PDF Visual Analysis**
   - Upload a slide deck PDF
   - LOOK FOR:
     - "Extracted X page images from PDF" message
     - Visual elements described in output
     - Slide-by-slide analysis

2. **Claims Extraction**
   - Upload any presentation
   - LOOK FOR:
     - Primary thesis identified
     - Key claims numbered
     - Evidence anchors listed
     - Presenters extracted

3. **PWS Methodology Section**
   - Upload a business proposal PDF
   - LOOK FOR:
     - "ðŸ§ª PWS METHODOLOGY ANALYSIS" section
     - Stated assumptions listed
     - Hidden assumptions surfaced
     - PWS quality scores (X/10)
     - Suggested next steps

4. **Content Signals**
   - LOOK FOR at bottom of analysis:
     - "ðŸ“Š Data present" if statistics found
     - "ðŸ“š Sources cited" if references found
     - "ðŸŽ¯ PWS elements" if problems/assumptions detected
     - "ðŸ”® Forward-looking" if trends/predictions present

5. **Coaching Hint**
   - If student jumps to solutions without defining problem:
   - LOOK FOR: "ðŸ’¡ COACHING INSIGHT" section
   - Should redirect to problem definition

**Note:** Requires `pdf2image` and `poppler-utils` installed on server for visual extraction. Falls back to text-only if unavailable.

---

## ðŸ”¬ EDGE CASE TESTS

* Lawrence first message â†’ No ðŸ§  panel? âœ…
* Switch bots mid-chat â†’ Context preserved? âœ…
* No API key? â†’ Error shown, others work? âœ…
* Grading bot + no document â†’ Prompts for upload? âœ…
* PDF without images installed â†’ Falls back to text? âœ…
* Bias detection fails â†’ Blocks grading with explanation? âœ…

---

## âœ… QUICK TEST CHECKLIST

**Original Features:**
* [ ] TTA â†’ Question â†’ ðŸ§  Panel visible?
* [ ] "Chaos" prompt â†’ Red Team suggested?
* [ ] "AI will replace jobs" â†’ BQ triggered?
* [ ] Red Team â†’ ðŸ”´ Mode â†’ Brutal feedback?
* [ ] Deep Research â†’ Scores and sources?
* [ ] Next Phase â†’ % complete? Override?
* [ ] Show Example 3x â†’ Unique outputs?
* [ ] Voice input â†’ Transcribed & processed?

**Grading Agent (NEW):**
* [ ] Grading bot â†’ Welcome message with methodology table?
* [ ] Upload PDF â†’ Full grading pipeline runs?
* [ ] Bias detection â†’ 7 biases checked?
* [ ] Grade output â†’ Letter grade + breakdown table?
* [ ] Export JSON â†’ Structured data available?
* [ ] Explain Grade â†’ Rationale expanded?
* [ ] Quick Grade â†’ Faster result with verdict?

**Presentation Analyzer (NEW):**
* [ ] PDF upload â†’ Page images extracted?
* [ ] Claims â†’ Primary thesis identified?
* [ ] PWS Section â†’ Assumptions listed?
* [ ] Quality Scores â†’ Problem clarity shown?
* [ ] Coaching Hint â†’ Appears when needed?

---

## ðŸ›  HOW TO REPORT BUGS

Send to **Sagir** with:

1. Feature (include "Grading Agent" or "Presentation Analyzer" if applicable)
2. Bot
3. What you typed / uploaded
4. What you expected
5. What happened
6. Screenshot

---

## ðŸš« Known Issues (Don't Report)

* OAuth off
* ðŸ§  Panel = Workshop bots only
* Research errors if no API key
* PDF visual extraction requires poppler-utils (text fallback works)
* Grading requires gemini-3-flash-preview model access
* Quick grade is simplified (no bias detection)

---

## ðŸ§ª NEW DEPENDENCIES TO VERIFY

For full functionality, ensure these are configured:

| Dependency | Feature | Fallback |
|------------|---------|----------|
| pdf2image + poppler | PDF visual analysis | Text-only extraction |
| Neo4j connection | Grading hidden connections | Reduced MT score |
| FileSearch RAG | Framework validation | Limited context |
| LangExtract | PWS pattern detection | Basic extraction |
| gemini-3-flash-preview | Complex grading reasoning | gemini-2.0-flash |

---

### CRITICAL TAKEAWAY

**Every new feature is a lens on thinking.**
You're not just testing functionality â€” you're testing cognition.

The Grading Agent asks: "Did they find REAL problems?"
The Presentation Analyzer asks: "What claims are being made, and are they grounded?"

Both are about **truth-seeking over confirmation**.

---

### NEXT STEP

**Your mission**: Break it. Stretch it. Confuse it.
Because *only in the breakdown* can we understand what really works.

Upload the worst student work you can find.
Upload a presentation full of assumptions.
See what the system catches â€” and what it misses.

**That's how we learn.**
