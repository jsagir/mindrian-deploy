What's Live Right Now â€” Full Branch Release Notes
================================================================
Branch: claude/review-codebase-nRTQ3
Date: January 29, 2026
Total: 20 commits, +10,500 lines, 37 files changed

================================================================
CHANGES AND HOW THEY ADDRESS FEEDBACK
================================================================

1. "Research gives random results, not relevant to my problem" (Sagir)
   â†’ Fixed. Research now uses AI to filter and score every result.
   5-phase workflow: Query Decomposition â†’ Discovery â†’ Source Evaluation â†’ Gap Analysis â†’ Synthesis.
   Every result gets a relevance score (0.0-1.0) and is framed for PWS methodology.
   Sources ranked by authority: .gov (0.98) > .edu (0.90) > news (0.75) > blogs (0.55).
   Bad results filtered out before you see them.

2. "I don't know which bot to use for my type of problem" (Team)
   â†’ Fixed. System now detects problem complexity using Cynefin framework.
   Simple problems (clear cause-effect) â†’ suggests Ackoff (validate with data).
   Complicated problems (expertise needed) â†’ suggests S-Curve, JTBD.
   Complex problems (probe and learn) â†’ suggests TTA, Scenario Analysis.
   Chaotic problems (act fast) â†’ suggests Red Team, Known Unknowns.
   This happens automatically based on what you type.

3. "Beautiful Questions should pop up when I need them" (Edwards)
   â†’ Done. System detects your conversation phase and suggests BQ bot:
   Lots of certainty but no clear problem â†’ suggests WHY questions.
   Problem defined, exploring options â†’ suggests WHAT IF questions.
   Solutions ready, planning action â†’ suggests HOW questions.
   You don't have to remember to switch â€” it nudges you.

4. "Red Team is too balanced sometimes, I want brutal feedback" (Sagir)
   â†’ Done. New "Extreme Opposition" toggle button in Red Team.
   Click it â†’ pure attack mode. No balanced views. No encouragement. No mercy.
   Finds the single most devastating flaw. Plays strategic competitor.
   Click again â†’ back to normal balanced devil's advocate mode.
   Button appears in Red Team actions automatically.

5. "Next Phase button doesn't check if I'm ready" (Lawrence)
   â†’ Fixed. Clicking Next Phase now validates completion first.
   Shows percentage complete and what you're missing.
   Gives clear instructions for the next phase.
   Fetches current news and trends for your specific topic.
   "Proceed Anyway" button if you want to skip ahead.

6. "I lose my progress when switching bots" (Team)
   â†’ Already working. Switch from Lawrence to Red Team and back.
   Your full conversation history is preserved.
   Phase progress saved. Context handed off between bots.
   Works across browser refreshes too (PostgreSQL persistence).

7. "Show Example gives the same example every time" (Lawrence)
   â†’ Fixed. Examples now come from 3 sources in priority order:
   1. Neo4j LazyGraph (8,400+ real case studies)
   2. File Search RAG (course materials)
   3. Tavily web search (fresh examples)
   Tracks what you've seen so you don't get repeats.

8. "I want to see the bot's thinking process" (Edwards)
   â†’ Done. Chain-of-thought steps now visible in UI.
   Spinners show: "Checking Phase Completion", "Preparing Next Phase", "Researching".
   Collapsible steps show the reasoning. Toggle in settings if you want to hide.

9. "OAuth login for tracking users" (Sagir)
   â†’ Ready. Google and GitHub OAuth callbacks implemented.
   Requires CHAINLIT_AUTH_SECRET on Render (now set).
   Callback URLs configured. Just need to enable in Google Cloud Console.

10. "Multi-agent analysis is too hidden" (Edwards)
    â†’ Now available via button in Larry Playground.
    Quick Analysis, Research + Explore, Validated Decision, Full Analysis modes.
    Runs multiple bots in sequence, shows results with cl.Step visualization.
    Hidden from Lawrence (simple mode) to reduce clutter.

11. "Need persistent memory across conversations" (Team)
    â†’ Done. LangGraph Memory provides persistent conversation context.
    State management across agent transitions.
    Conversation history preserved in graph state.
    Enables multi-turn reasoning across bot switches.
    Access via: /langgraph-memory or Multi-Agent Analysis button.

================================================================
NEW BOTS ADDED
================================================================

Beautiful Questions Bot
- WHY â†’ WHAT IF â†’ HOW methodology (Warren Berger)
- 3-phase workshop structure
- Auto-suggested based on conversation signals
- Helps reframe problems as powerful questions

Multi-Perspective Validation Bot
- Extracts domains from your problem
- Constructs expert personas
- Runs parallel research from each perspective
- Structured debate format
- Generates validation report

Domain Explorer Bot
- Analyze CV/resume to find innovation domains
- Analyze research papers for opportunities
- Score domains by PWS criteria
- Works with document upload

================================================================
NEW TOOLS ADDED
================================================================

Research Orchestrator (tools/research_orchestrator.py)
- 5-phase Tavily workflow
- Query decomposition into sub-queries
- Source evaluation and ranking
- Gap analysis for missing information
- Final synthesis with PWS framing

Result Synthesizer (tools/result_synthesizer.py)
- AI-powered relevance scoring
- Filters irrelevant results before display
- Frames results for current bot/methodology
- Configurable thresholds per bot

Phase Validator (tools/phase_validator.py)
- Checks workshop phase completion
- Uses LangExtract patterns
- Returns missing deliverables
- Generates guidance for completion

Phase Enricher (tools/phase_enricher.py)
- Fetches news for user's domain
- Fetches Google Trends data
- Contextualizes phase transitions
- Domain-specific examples

Validation Workflow (tools/validation_workflow.py)
- 6-phase agentic validation
- Runs automatically through all phases
- Extracts insights at each step
- Generates comprehensive report

API Validator (scripts/validate_apis.py)
- Tests all 12+ API integrations
- Color-coded pass/fail output
- Suggests fixes for failures
- Run with: python scripts/validate_apis.py

LangGraph Memory (agents/multi_agent_graph.py)
- Persistent conversation context with LangGraph
- State management across agent transitions
- Conversation history preserved in graph state
- Enables multi-turn reasoning across bot switches
- Access via: /langgraph-memory or Multi-Agent Analysis button

Extended Thinking UI (public/elements/ThinkingPanel.jsx)
- Visualizes LLM reasoning steps in collapsible panel
- Shows thinking BEFORE response is generated
- Detects assumptions, solution-jumping
- Applies methodology-specific reasoning
- Works across all workshop bots (not simple Lawrence)

Voice & Real-Time Audio (utils/realtime_stt.py, utils/elevenlabs_streaming.py)
- Real-time speech-to-text via Deepgram WebSocket
- Fallback to Gemini transcription if Deepgram unavailable
- ElevenLabs TTS with streaming audio output
- Sub-second response time when Deepgram enabled
- 3,000+ voice options for brand consistency

================================================================
WHAT TO TEST
================================================================

Open Mindrian in incognito â†’ is Lawrence the default?

Type "I want to start a plant-based meat company" â†’ does Research give relevant, scored results?

Type "My startup is in chaos, everything is breaking" â†’ does it suggest Red Team? (Cynefin: chaotic)

Type "I'm absolutely certain AI will replace all jobs" â†’ does it suggest Beautiful Questions? (high certainty, no problem)

Select Red Team â†’ is "Extreme Opposition" button visible?

Click Extreme Opposition â†’ type an idea â†’ is critique brutal and one-sided?

Select TTA bot â†’ type a question â†’ do you see "ðŸ§  TTA's Thinking" panel before response?

Click the microphone icon â†’ speak a question â†’ does it transcribe and respond?

Select Scenario Analysis â†’ type a topic â†’ click Next Phase â†’ does it validate completion?

Switch to JTBD â†’ switch back to Scenario â†’ is your phase progress saved?

Click Show Example 3 times â†’ are examples different each time?

================================================================
KNOWN ISSUES (DON'T REPORT THESE)
================================================================

Cynefin boosting requires Neo4j connection.
If Neo4j not connected, agent suggestions still work (just no domain boost).

Some research buttons may error if API keys not set on Render.
Buttons degrade gracefully â€” they just won't return data.

Beautiful Questions suggestions depend on LangExtract patterns.
May not trigger on very short or vague messages.

Extreme Opposition stays on until you toggle it off.
Switching away from Red Team and back resets to normal mode.

OAuth login shows but may not work until Google Cloud Console is fully configured.
App works fine without login â€” just no user tracking.

Voice input is experimental.
Works but may have latency issues.

PDF images not extracted (text only for now).

================================================================
API KEYS ON RENDER
================================================================

REQUIRED:
  GOOGLE_API_KEY âœ“

FOR VOICE:
  ELEVENLABS_API_KEY âœ“
  ELEVENLABS_VOICE_ID âœ“

FOR RESEARCH:
  TAVILY_API_KEY âœ“

FOR PERSISTENCE:
  CHAINLIT_DATABASE_URL âœ“
  CHAINLIT_AUTH_SECRET âœ“ (just added)

FOR GRAPH FEATURES:
  NEO4J_URI
  NEO4J_USERNAME
  NEO4J_PASSWORD

FOR EXTENDED RESEARCH:
  SERPAPI_API_KEY (Google Trends, Patents)
  FRED_API_KEY (economic data)
  BLS_API_KEY (labor statistics)

================================================================
FILES CHANGED (37 files, +10,500 lines)
================================================================

New Features:
  tools/research_orchestrator.py      - 5-phase research workflow
  tools/result_synthesizer.py         - AI relevance scoring
  tools/phase_validator.py            - phase completion checking
  tools/phase_enricher.py             - news/trends fetching
  tools/validation_workflow.py        - 6-phase agentic validation
  prompts/beautiful_question.py       - new bot (496 lines)
  prompts/scenario_phases.py          - phase definitions
  prompts/redteam.py                  - added Extreme Opposition mode
  agents/multi_agent_graph.py         - LangGraph orchestration
  scripts/validate_apis.py            - API testing script

UI Components:
  public/elements/DIKWPyramid.jsx
  public/elements/PhaseProgress.jsx
  public/elements/ResearchMatrix.jsx
  public/elements/ScenarioSetupForm.jsx
  public/elements/ProblemDefinitionForm.jsx

Main App:
  mindrian_chat.py                    - Cynefin boost, BQ triggers, phase UX (+1,800 lines)

Documentation:
  R&D/16_v4_architecture/README.md
  qa/2026-01-29/*.md (5 QA docs)
  chainlit.md (rewritten)
  CLAUDE.md (updated)

================================================================
HOW TO DEPLOY
================================================================

Currently deployed: claude/review-codebase-nRTQ3

TO REVERT TO MAIN:
  Render Dashboard â†’ Settings â†’ Build & Deploy
  Change Branch to: main
  Save â†’ Auto-deploys

TO MERGE TO MAIN (permanent):
  git checkout main
  git merge claude/review-codebase-nRTQ3
  git push origin main

================================================================
FEEDBACK
================================================================

Drop your feedback in the group chat or send to Sagir:
  - Which bot
  - What you typed
  - What happened vs. expected
  - Screenshot if you can

================================================================
END OF RELEASE NOTES
================================================================
